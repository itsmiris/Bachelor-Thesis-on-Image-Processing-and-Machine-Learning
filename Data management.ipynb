{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training set and val set\n",
      "Train set size :  (14800,)\n",
      "Val set size :  (200,)\n",
      "Train and validation datasets ready !\n"
     ]
    }
   ],
   "source": [
    "#obtinerea scorurilor de atractivitate ca medie a notelor date \n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# path to the images and the text file which holds the scores and ids\n",
    "base_images_path = r'C:\\Users\\user\\Desktop\\licenta\\15000\\AVA_dataset\\imageslucru\\\\'\n",
    "ava_dataset_path = r'C:\\Users\\user\\Desktop\\licenta\\15000\\AVA_dataset\\AVAlucru.txt'\n",
    "\n",
    "files = glob.glob(base_images_path + \"*.jpg\")\n",
    "files = sorted(files)\n",
    "\n",
    "train_image_paths = []\n",
    "train_scores = []\n",
    "\n",
    "pathToPhotos = \"C:\\\\Users\\\\user\\\\Desktop\\\\licenta\\\\15000\\\\AVA_dataset\\\\imageslucru\"\n",
    "s=[]\n",
    "for filename in os.listdir(pathToPhotos):\n",
    "    getName = filename.split('.')\n",
    "    s.append(int(getName[0]))\n",
    "s.sort()\n",
    "#print(s)\n",
    "\n",
    "print(\"Loading training set and val set\")\n",
    "with open(ava_dataset_path, mode='r') as f:\n",
    "    lines = f.readlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        token = line.split()\n",
    "        nume = int(token[1])\n",
    "        values = np.array(token[2:12], dtype='float32')\n",
    "        score = 0.0\n",
    "        for i in range(len(values)):\n",
    "            score += values[i]*(i+1)\n",
    "        score /= values.sum()\n",
    "        score = round(score,2)\n",
    "        #print(score)\n",
    "\n",
    "        file_path = base_images_path + str(nume) + '.jpg'\n",
    "        if os.path.exists(file_path):\n",
    "            train_image_paths.append(file_path)\n",
    "            #train_scores.append(score)\n",
    "        g = open(\"scores.txt\", \"a\")\n",
    "        g.write(str(score)+'\\n')\n",
    "        g.close()\n",
    "         \n",
    "train_image_paths = np.array(train_image_paths)\n",
    "#train_scores = np.array(train_scores, dtype='float32')\n",
    "\n",
    "val_image_paths = train_image_paths[-200:]\n",
    "#val_scores = train_scores[-200:]\n",
    "train_image_paths = train_image_paths[:-200]\n",
    "#train_scores = train_scores[:-200]\n",
    "\n",
    "print('Train set size : ', train_image_paths.shape)\n",
    "print('Val set size : ', val_image_paths.shape)\n",
    "print('Train and validation datasets ready !')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size :  (14800,) (14800,)\n",
      "Val set size :  (200,) (200,)\n",
      "Train and validation datasets ready !\n"
     ]
    }
   ],
   "source": [
    "#obtinerea etichetelor pozelor folosite, din toate calculate\n",
    "train_scores = []\n",
    "with open('scores.txt', mode='r') as l:\n",
    "    liness = l.readlines()\n",
    "    for i, line in enumerate(liness):\n",
    "        for j in range(0,15000):\n",
    "            if(s[j]==i):\n",
    "                k = open(\"scoresfinal.txt\", \"a\")\n",
    "                k.write(str(liness[i-1]))\n",
    "                k.close()\n",
    "                train_scores.append(liness[i-1])\n",
    "                \n",
    "    k = open(\"scoresfinal.txt\", \"a\")\n",
    "    k.write(str(liness[-1]))\n",
    "    k.close()\n",
    "    train_scores.append(liness[-1])\n",
    "    \n",
    "train_scores = np.array(train_scores, dtype='float32')\n",
    "val_scores = train_scores[-200:]\n",
    "train_scores = train_scores[:-200]\n",
    "\n",
    "print('Train set size : ', train_image_paths.shape, train_scores.shape)\n",
    "print('Val set size : ', val_image_paths.shape, val_scores.shape)\n",
    "print('Train and validation datasets ready !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000,)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "from past.builtins import xrange\n",
    "import sys\n",
    "#obtinerea descriptorilor tuturor celor 15000 de imagini\n",
    "base_images_path = r'C:\\Users\\user\\Desktop\\licenta\\15000\\AVA_dataset\\imageslucru\\\\'\n",
    "ava_dataset_path = r'C:\\Users\\user\\Desktop\\licenta\\15000\\AVA_dataset\\AVAlucru.txt'\n",
    "\n",
    "files = glob.glob(base_images_path + \"*.jpg\")\n",
    "files = sorted(files)\n",
    "\n",
    "train_image_paths = []\n",
    "with open(ava_dataset_path, mode='r') as f:\n",
    "    lines = f.readlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        token = line.split()\n",
    "        id = int(token[0])\n",
    "        \n",
    "        file_path = base_images_path + str(id) + '.jpg'\n",
    "        if os.path.exists(file_path):\n",
    "            train_image_paths.append(file_path)\n",
    "            \n",
    "train_image_paths = np.array(train_image_paths)\n",
    "print(train_image_paths.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_descriptor(img):\n",
    "    hsv_image = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    levels = 8\n",
    "    hhist = cv2.calcHist([hsv_image],[0], None, [levels], [0, 180] )\n",
    "    sumhhist = np.sum(hhist)\n",
    "    hhist[:] = [x/sumhhist for x in hhist]\n",
    "    shist = cv2.calcHist([hsv_image],[1], None, [levels], [0, 255] )\n",
    "    sumshist = np.sum(shist)\n",
    "    shist[:] = [x/sumshist for x in shist]\n",
    "    vhist = cv2.calcHist([hsv_image],[2], None, [levels], [0, 255] )\n",
    "    sumvhist = np.sum(vhist)\n",
    "    vhist[:] = [x/sumvhist for x in vhist]\n",
    "    colordescriptor = np.concatenate([hhist, shist, vhist]).flatten()\n",
    "    return colordescriptor\n",
    "\n",
    "def contrast_descriptor(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    layer=img.copy()\n",
    "    gp=[layer]\n",
    "    descriptor_contrast = 0\n",
    "    for i in range(6):\n",
    "        layer=cv2.pyrDown(layer)\n",
    "        gp.append(layer)\n",
    "    layer=gp[5]\n",
    "    lap_pyr=[layer]\n",
    "    for i in range(5,0,-1):\n",
    "        size=(gp[i-1].shape[1],gp[i-1].shape[0])\n",
    "        gaussian_extend=cv2.pyrUp(gp[i],dstsize=size)\n",
    "        laplacian=cv2.subtract(gp[i-1],gaussian_extend)\n",
    "        lap_pyr.append(laplacian)\n",
    "    for i in range(1,6):\n",
    "        m,n = lap_pyr[i].shape\n",
    "        contrast_lvl = 0\n",
    "        for j in range (m):\n",
    "            for k in range (n):\n",
    "                contrast_lvl  = contrast_lvl + int(lap_pyr[i][j][k])*int(lap_pyr[i][j][k])\n",
    "        contrast_lvl = contrast_lvl/(m*n*np.power(2,5-i))\n",
    "        descriptor_contrast = descriptor_contrast + contrast_lvl\n",
    "    return(descriptor_contrast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 25)\n"
     ]
    }
   ],
   "source": [
    "descriptori_img=np.zeros((15000,25))\n",
    "#matricea trb sa aiba 15000 linii si 25 col\n",
    "m=15000;\n",
    "n=25;\n",
    "for i in range(m):\n",
    "    img=cv2.imread(train_image_paths[i])\n",
    "    for j in range(24):\n",
    "        descriptori_img[i][j]=(color_descriptor(img))[j]\n",
    "    descriptori_img[i][n-1]=contrast_descriptor(img)\n",
    "print(descriptori_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train descriptors ready!\n"
     ]
    }
   ],
   "source": [
    "for i in range(m):\n",
    "    for j in range(n):\n",
    "        k = open(\"image_descriptor.txt\", \"a\")\n",
    "        k.write(str(descriptori_img[i][j])+' ')\n",
    "    k.write('\\n')\n",
    "    k.close()\n",
    "print('Train descriptors ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
